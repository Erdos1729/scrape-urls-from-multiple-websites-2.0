## scrape urls from multiple websites 2.0
 
This repository will help you to extract all the urls reported on a website and save it in an excel file. This code works on multiple weblinks provided in a csv file and thus saves you a lot of manual work. If you are working on scouting multiple websites for identifying press releases, presentations, annual reports etc., this code will come in handy and save alot of man-hours.

## Instructions

-   pip install -r requirements
-   Run url_extract_2.0.py

## Reference

I devised the solution from the following pages of the documentation:

-   [![Urllib](https://docs.python.org/3/library/urllib.html#:~:text=urllib%20is%20a%20package%20that%20collects%20several%20modules%20for%20working%20with%20URLs%3A&text=request%20for%20opening%20and%20reading,the%20exceptions%20raised%20by%20urllib.)] package that collects several modules for working with URLs
-   [![beautyfulsoup4](https://pypi.org/project/beautifulsoup4/)] to scrape information from web pages
-   [![feedparser](https://pypi.org/project/feedparser/)] to parse RSS feeds in Python
-   [![pandas](https://pypi.org/project/pandas/)] for data structuring


